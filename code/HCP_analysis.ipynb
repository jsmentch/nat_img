{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informal-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "urban-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(921, 170494)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load a cifti image\n",
    "input1='../sourcedata/data/HCP_7T_movie_FIX/brain/HCP_7T_movie_FIX/100610/MNINonLinear/Results/tfMRI_MOVIE1_7T_AP/tfMRI_MOVIE1_7T_AP_Atlas_1.6mm_MSMAll_hp2000_clean.dtseries.nii'\n",
    "tr=1\n",
    "img = nb.load(input1)\n",
    "X = img.get_fdata()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comic-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a feature\n",
    "input_f=np.load('../sourcedata/data/HCP_7T_movie_FIX/features/7T_MOVIE1_CC1_v2_mfs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "biological-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_f=np.load('../sourcedata/data/HCP_7T_movie_FIX/features/7T_MOVIE1_CC1_v2_pycochleagram_6_hrf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "heated-speech",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1474)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = img.get_fdata() #load data from nii\n",
    "X_t = X[17:] #trim beginning, first 17 TRs\n",
    "Ybrain = X_t[:1009] #trim end to end of film    braintrain.append(s_brain[:-200,:]) #roughly 80 20 split, trim the last 200 TRs of each subject to save as test set\n",
    "Ybrain = Ybrain[:,union_ind_ind[0]]\n",
    "#     Ybrain_train = s_brain[:-200,:]\n",
    "#     Ybrain_test = s_brain[-200:,:]\n",
    "#as in https://nilearn.github.io/auto_examples/02_decoding/plot_miyawaki_encoding.html#sphx-glr-auto-examples-02-decoding-plot-miyawaki-encoding-py\n",
    "Ybrain = np.nan_to_num(Ybrain)\n",
    "estimator = RidgeCV(alphas=[0.1, 1.0, 10.0, 100])\n",
    "cv = KFold(n_splits=5)\n",
    "scores = []\n",
    "for train, test in cv.split(X=X_as):\n",
    "    train = train[2:-2] #remove the first and last 3 seconds of each test and train partition\n",
    "    test = test[2:-2]\n",
    "    #print('train, test')\n",
    "    # we train the Ridge estimator on the training set\n",
    "    # and predict the fMRI activity for the test set\n",
    "    predictions = estimator.fit(X_as.reshape(-1, X_as.shape[1])[train], Ybrain[train]).predict(\n",
    "        X_as.reshape(-1, X_as.shape[1])[test])\n",
    "    # we compute how much variance our encoding model explains in each voxel\n",
    "    scores.append(r2_score(Ybrain[test], predictions,\n",
    "                           multioutput='raw_values'))\n",
    "cut_score = np.mean(scores, axis=0)\n",
    "#cut_score[cut_score < 0] = 0\n",
    "\n",
    "cut_score_new = np.zeros((union_ind.shape[0]))\n",
    "for i,ind in enumerate(union_ind_ind[0]):\n",
    "    cut_score_new[ind]=cut_score[i]\n",
    "all_scores.append(cut_score_new)\n",
    "#make a temp valid cifti2img\n",
    "#temp_img = nb.Cifti2Image(cut_score.reshape((1,91282)), img.header)\n",
    "plot_surf(hcp.mesh.flat,\\\n",
    "    hcp.cortex_data(cut_score_new), cmap='viridis',symmetric_cmap=False, vmin=0, vmax=0.25, colorbar=True,avg_method='median',#figure=fig,\\\n",
    "    bg_map=hcp.mesh.sulc,title=f'voxel-wise Encoding Model, R2 {sub} 5-fold cv {featname}',\\\n",
    "    data_alpha=hcp.cortex_data(np.ones(cut_score_new.shape)),#np.ones(data_alpha05.shape)),\\\n",
    "    data_remove=hcp.cortex_data(data_remove),output_file=f'2021_02_05_low_level/{sub}-{featname}_ridgecv.png')\n",
    "#plot_dscalar(temp_img, title=f'voxel-wise Encoding Model, R2 {sub} 5-fold cv {featname}',colorbar=True, vmin=0, vmax=1, plot_abs=False, threshold=None, cmap='bwr', output_file=f'2021_02_05_low_level/{sub}-{featname}_ridgecv.png')\n",
    "all_scores=np.asarray(all_scores)\n",
    "all_scores_mean=np.mean(all_scores,axis=0)\n",
    "plot_surf(hcp.mesh.flat,\\\n",
    "hcp.cortex_data(all_scores_mean), cmap='viridis',symmetric_cmap=False, vmin=0, vmax=0.25, colorbar=True,avg_method='median',#figure=fig,\\\n",
    "bg_map=hcp.mesh.sulc,title=f'voxel-wise Encoding Model, R2 {sub} 5-fold cv {featname}',\\\n",
    "data_alpha=hcp.cortex_data(np.ones(cut_score_new.shape)),#np.ones(data_alpha05.shape)),\\\n",
    "data_remove=hcp.cortex_data(data_remove),output_file=f'2021_02_05_low_level/all-{featname}_ridgecv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-closing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-planner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-department",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-aberdeen",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

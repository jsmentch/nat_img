{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a very simple LSTM model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how long this will take to run for 7t data with a not too complicated model and what kind of resources we are talking about - single run single subject.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import hrf_tools\n",
    "#from analysis import load_data_HCP_MMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_HCP_MMP(subject,feature,n_movies):\n",
    "    # Inputs: subject = HCP id eg 100610\n",
    "    #         feature='mfs'\n",
    "    #         n_movies is a list of movie indices 1 thru 4\n",
    "    # Returns: X feature data (2D; time x feature)\n",
    "    #          Y brain data (2D; time x grayordinate)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y_l=[]\n",
    "    x_l=[]\n",
    "    stim = ['tfMRI_MOVIE1_7T_AP','tfMRI_MOVIE2_7T_PA','tfMRI_MOVIE3_7T_PA','tfMRI_MOVIE4_7T_AP']\n",
    "    stim_feat = ['7T_MOVIE1_CC1_v2', '7T_MOVIE2_HO1_v2', '7T_MOVIE3_CC2_v2', '7T_MOVIE4_HO2_v2']\n",
    "    slice_starts = [\n",
    "        [20,284, 526,734],\n",
    "        [20,267,545],\n",
    "        [20,221,425,649],\n",
    "        [20,272,522]]\n",
    "    slice_stops =  [\n",
    "        [264,506,714,798],\n",
    "        [247,525,795],\n",
    "        [201,405,629,792],\n",
    "        [252,502,777]]\n",
    "    for i in n_movies:\n",
    "        i=i-1\n",
    "        exclude_final=slice_stops[i][-1] #trim the final movie since it is in all scans\n",
    "        #load brain image\n",
    "        im_file = f'../sourcedata/data/HCP_7T_movie_FIX/brain/parcellations/parcellated/sub{str(subject)}_{stim[i]}.ptseries.nii'\n",
    "        img = nb.load(im_file)\n",
    "        img_y = img.get_fdata()\n",
    "        img_y = scaler.fit_transform(img_y)\n",
    "\n",
    "        #load feature\n",
    "        feat_x = np.load(f'../sourcedata/data/HCP_7T_movie_FIX/features/{stim_feat[i]}_{feature}.npy')\n",
    "        feat_x = resample(feat_x, img_y.shape[0], axis=0) #resample to 1hz for now \n",
    "        #feat_x=feat_x.T\n",
    "        #trim final movies\n",
    "        img_y = img_y[:exclude_final,:]\n",
    "        feat_x = feat_x[:exclude_final,:]\n",
    "        y_l.append(img_y)\n",
    "        x_l.append(feat_x)\n",
    "    Y=np.vstack(y_l)\n",
    "    X=np.vstack(x_l)\n",
    "    #X = scaler.fit_transform(X)\n",
    "    #vertex_info = hcp.get_HCP_vertex_info(img)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10957096950636860581\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23552872960\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1229633727219759524\n",
      "physical_device_desc: \"device: 0, name: Quadro RTX 6000, pci bus id: 0000:5e:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=100610\n",
    "feature='as_scores'\n",
    "n_movies=[1,2,3]\n",
    "X,Y = load_data_HCP_MMP(subject,feature,n_movies)\n",
    "\n",
    "X_test,Y_test = load_data_HCP_MMP(subject,feature,[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hrf_tools.apply_optimal_hrf_10hz(X,1)\n",
    "X_test = hrf_tools.apply_optimal_hrf_10hz(X_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2385, 521)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[:-500,:]\n",
    "Y_val = Y[:-500,:]\n",
    "X_train = X[-500:,:]\n",
    "Y_train = Y[-500:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 360)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[-500:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "time_steps = 10\n",
    "input_units = 521\n",
    "input_shape = (time_steps,input_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/api/preprocessing/timeseries/\n",
    "dataset_train = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_train, \n",
    "                                                     targets = Y_train, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)\n",
    "dataset_val = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_val, \n",
    "                                                     targets = Y_val, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)\n",
    "\n",
    "dataset_test = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_test, \n",
    "                                                     targets = Y_test, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### design a simple(?) lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_6 (Dropout)          (None, 10, 521)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 521)               2173612   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 521)               271962    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 521)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 521)               271962    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 521)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 360)               187920    \n",
      "=================================================================\n",
      "Total params: 2,905,456\n",
      "Trainable params: 2,905,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "#model.add(layers.Embedding(input_dim=1024, output_dim=1024))\n",
    "\n",
    "model.add(keras.Input(shape=input_shape)) #omit beacuse of bug\n",
    "model.add(layers.Dropout(0.1))\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(input_units))#,batch_input_shape=(batch_size, time_steps, input_units)))\n",
    "model.add(layers.Dense(521,activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(521,activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "#model.add(layers.Dense(360))\n",
    "\n",
    "#model.add(layers.UpSampling1D(size=10))\n",
    "#model.add(layers.Dense(17049))\n",
    "#model.add(layers.Dense(128))\n",
    "model.add(layers.Dense(360,activation='relu'))\n",
    "#model.add(layers.Dense(360))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(x_train,0), np.expand_dims(y_train,0)))\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_hrf, Y))\n",
    "\n",
    "# dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Buffer size to shuffle the dataset\n",
    "# # (TF data is designed to work with possibly infinite sequences,\n",
    "# # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# # it maintains a buffer in which it shuffles elements).\n",
    "# BUFFER_SIZE = 10000\n",
    "\n",
    "# dataset = (\n",
    "#     dataset\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(batch_size, drop_remainder=True)\n",
    "#     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#simple_rnn = layers.SimpleRNN(18, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_sequence_output, final_state = simple_rnn(np.expand_dims(x_train,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, restore_best_weights=True, patience=5)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
    "    optimizer=\"adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "38/38 [==============================] - 3s 34ms/step - loss: 0.9095 - val_loss: 0.9216\n",
      "Epoch 2/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.9089 - val_loss: 0.9209\n",
      "Epoch 3/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.9075 - val_loss: 0.9168\n",
      "Epoch 4/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.9046 - val_loss: 0.9215\n",
      "Epoch 5/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.9067 - val_loss: 0.9124\n",
      "Epoch 6/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.9028 - val_loss: 0.9090\n",
      "Epoch 7/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8962 - val_loss: 0.9203\n",
      "Epoch 8/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.9007 - val_loss: 0.9175\n",
      "Epoch 9/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.9018 - val_loss: 0.9132\n",
      "Epoch 10/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8980 - val_loss: 0.9112\n",
      "Epoch 11/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8957 - val_loss: 0.9058\n",
      "Epoch 12/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8919 - val_loss: 0.9036\n",
      "Epoch 13/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8883 - val_loss: 0.9099\n",
      "Epoch 14/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8894 - val_loss: 0.8961\n",
      "Epoch 15/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8878 - val_loss: 0.9017\n",
      "Epoch 16/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8915 - val_loss: 0.8969\n",
      "Epoch 17/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8828 - val_loss: 0.8995\n",
      "Epoch 18/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8763 - val_loss: 0.9061\n",
      "Epoch 19/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8861 - val_loss: 0.8912\n",
      "Epoch 20/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8799 - val_loss: 0.8972\n",
      "Epoch 21/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8776 - val_loss: 0.8931\n",
      "Epoch 22/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8682 - val_loss: 0.8936\n",
      "Epoch 23/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8714 - val_loss: 0.8831\n",
      "Epoch 24/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8644 - val_loss: 0.9009\n",
      "Epoch 25/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8656 - val_loss: 0.8809\n",
      "Epoch 26/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8680 - val_loss: 0.8866\n",
      "Epoch 27/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8685 - val_loss: 0.8789\n",
      "Epoch 28/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8579 - val_loss: 0.8896\n",
      "Epoch 29/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8582 - val_loss: 0.8694\n",
      "Epoch 30/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8525 - val_loss: 0.8903\n",
      "Epoch 31/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8561 - val_loss: 0.8618\n",
      "Epoch 32/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8505 - val_loss: 0.8725\n",
      "Epoch 33/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8473 - val_loss: 0.8756\n",
      "Epoch 34/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8624 - val_loss: 0.8758\n",
      "Epoch 35/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8535 - val_loss: 0.8708\n",
      "Epoch 36/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8540 - val_loss: 0.8611\n",
      "Epoch 37/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8442 - val_loss: 0.8579\n",
      "Epoch 38/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8400 - val_loss: 0.8580\n",
      "Epoch 39/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8353 - val_loss: 0.8598\n",
      "Epoch 40/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8331 - val_loss: 0.8516\n",
      "Epoch 41/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8334 - val_loss: 0.8441\n",
      "Epoch 42/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8297 - val_loss: 0.8389\n",
      "Epoch 43/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8290 - val_loss: 0.8572\n",
      "Epoch 44/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8336 - val_loss: 0.8446\n",
      "Epoch 45/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8240 - val_loss: 0.8420\n",
      "Epoch 46/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8237 - val_loss: 0.8459\n",
      "Epoch 47/1000\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 0.8204 - val_loss: 0.8372\n",
      "Epoch 48/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8226 - val_loss: 0.8453\n",
      "Epoch 49/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8179 - val_loss: 0.8366\n",
      "Epoch 50/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8169 - val_loss: 0.8486\n",
      "Epoch 51/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8256 - val_loss: 0.8326\n",
      "Epoch 52/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8165 - val_loss: 0.8254\n",
      "Epoch 53/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8102 - val_loss: 0.8274\n",
      "Epoch 54/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8137 - val_loss: 0.8229\n",
      "Epoch 55/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8040 - val_loss: 0.8229\n",
      "Epoch 56/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8035 - val_loss: 0.8213\n",
      "Epoch 57/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8035 - val_loss: 0.8189\n",
      "Epoch 58/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.8011 - val_loss: 0.8164\n",
      "Epoch 59/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7990 - val_loss: 0.8096\n",
      "Epoch 60/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7928 - val_loss: 0.8100\n",
      "Epoch 61/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7942 - val_loss: 0.8240\n",
      "Epoch 62/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7944 - val_loss: 0.8164\n",
      "Epoch 63/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7961 - val_loss: 0.8041\n",
      "Epoch 64/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7929 - val_loss: 0.8175\n",
      "Epoch 65/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7912 - val_loss: 0.8106\n",
      "Epoch 66/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7938 - val_loss: 0.8079\n",
      "Epoch 67/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7937 - val_loss: 0.8138\n",
      "Epoch 68/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7896 - val_loss: 0.8037\n",
      "Epoch 69/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7814 - val_loss: 0.8027\n",
      "Epoch 70/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7816 - val_loss: 0.8217\n",
      "Epoch 71/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7883 - val_loss: 0.8132\n",
      "Epoch 72/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7861 - val_loss: 0.8011\n",
      "Epoch 73/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7834 - val_loss: 0.8106\n",
      "Epoch 74/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7799 - val_loss: 0.8010\n",
      "Epoch 75/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7819 - val_loss: 0.8105\n",
      "Epoch 76/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7843 - val_loss: 0.7920\n",
      "Epoch 77/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7755 - val_loss: 0.7973\n",
      "Epoch 78/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7757 - val_loss: 0.7881\n",
      "Epoch 79/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7696 - val_loss: 0.7807\n",
      "Epoch 80/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7654 - val_loss: 0.7823\n",
      "Epoch 81/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7669 - val_loss: 0.7802\n",
      "Epoch 82/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7632 - val_loss: 0.7803\n",
      "Epoch 83/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7611 - val_loss: 0.7946\n",
      "Epoch 84/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7710 - val_loss: 0.7765\n",
      "Epoch 85/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7639 - val_loss: 0.7823\n",
      "Epoch 86/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7669 - val_loss: 0.7799\n",
      "Epoch 87/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7608 - val_loss: 0.7755\n",
      "Epoch 88/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7597 - val_loss: 0.7685\n",
      "Epoch 89/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7537 - val_loss: 0.7710\n",
      "Epoch 90/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7528 - val_loss: 0.7725\n",
      "Epoch 91/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7529 - val_loss: 0.7683\n",
      "Epoch 92/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7497 - val_loss: 0.7637\n",
      "Epoch 93/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7495 - val_loss: 0.7644\n",
      "Epoch 94/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7497 - val_loss: 0.7716\n",
      "Epoch 95/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7570 - val_loss: 0.7847\n",
      "Epoch 96/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7522 - val_loss: 0.7725\n",
      "Epoch 97/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7484 - val_loss: 0.7582\n",
      "Epoch 98/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7423 - val_loss: 0.7623\n",
      "Epoch 99/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7457 - val_loss: 0.7573\n",
      "Epoch 100/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7487 - val_loss: 0.7574\n",
      "Epoch 101/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7438 - val_loss: 0.7531\n",
      "Epoch 102/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7453 - val_loss: 0.7499\n",
      "Epoch 103/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7382 - val_loss: 0.7540\n",
      "Epoch 104/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7391 - val_loss: 0.7471\n",
      "Epoch 105/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7348 - val_loss: 0.7541\n",
      "Epoch 106/1000\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.7355"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,epochs = 1000, validation_data=dataset_val, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../outputs/model.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../outputs/model.pb/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../outputs/model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../outputs/model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_test, \n",
    "                                                     targets = Y_test, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 18ms/step - loss: 2.2351 - accuracy: 0.0052 - mean_squared_error: 2.2351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.23513126373291, 0.0052083334885537624, 2.23513126373291]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 360)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataset_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777, 521)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a very simple LSTM model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how long this will take to run for 7t data with a not too complicated model and what kind of resources we are talking about - single run single subject.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import hrf_tools\n",
    "#from analysis import load_data_HCP_MMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_HCP_MMP(subject,feature,n_movies):\n",
    "    # Inputs: subject = HCP id eg 100610\n",
    "    #         feature='mfs'\n",
    "    #         n_movies is a list of movie indices 1 thru 4\n",
    "    # Returns: X feature data (2D; time x feature)\n",
    "    #          Y brain data (2D; time x grayordinate)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y_l=[]\n",
    "    x_l=[]\n",
    "    stim = ['tfMRI_MOVIE1_7T_AP','tfMRI_MOVIE2_7T_PA','tfMRI_MOVIE3_7T_PA','tfMRI_MOVIE4_7T_AP']\n",
    "    stim_feat = ['7T_MOVIE1_CC1_v2', '7T_MOVIE2_HO1_v2', '7T_MOVIE3_CC2_v2', '7T_MOVIE4_HO2_v2']\n",
    "    slice_starts = [\n",
    "        [20,284, 526,734],\n",
    "        [20,267,545],\n",
    "        [20,221,425,649],\n",
    "        [20,272,522]]\n",
    "    slice_stops =  [\n",
    "        [264,506,714,798],\n",
    "        [247,525,795],\n",
    "        [201,405,629,792],\n",
    "        [252,502,777]]\n",
    "    for i in n_movies:\n",
    "        i=i-1\n",
    "        exclude_final=slice_stops[i][-1] #trim the final movie since it is in all scans\n",
    "        #load brain image\n",
    "        im_file = f'../sourcedata/data/HCP_7T_movie_FIX/brain/parcellations/parcellated/sub{str(subject)}_{stim[i]}.ptseries.nii'\n",
    "        img = nb.load(im_file)\n",
    "        img_y = img.get_fdata()\n",
    "        img_y = scaler.fit_transform(img_y)\n",
    "\n",
    "        #load feature\n",
    "        feat_x = np.load(f'../sourcedata/data/HCP_7T_movie_FIX/features/{stim_feat[i]}_{feature}.npy')\n",
    "        feat_x = resample(feat_x, img_y.shape[0], axis=0) #resample to 1hz for now \n",
    "        #feat_x=feat_x.T\n",
    "        #trim final movies\n",
    "        img_y = img_y[:exclude_final,:]\n",
    "        feat_x = feat_x[:exclude_final,:]\n",
    "        y_l.append(img_y)\n",
    "        x_l.append(feat_x)\n",
    "    Y=np.vstack(y_l)\n",
    "    X=np.vstack(x_l)\n",
    "    #X = scaler.fit_transform(X)\n",
    "    #vertex_info = hcp.get_HCP_vertex_info(img)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10957096950636860581\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23552872960\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1229633727219759524\n",
      "physical_device_desc: \"device: 0, name: Quadro RTX 6000, pci bus id: 0000:5e:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=100610\n",
    "feature='as_scores'\n",
    "n_movies=[1,2,3]\n",
    "X,Y = load_data_HCP_MMP(subject,feature,n_movies)\n",
    "\n",
    "X_test,Y_test = load_data_HCP_MMP(subject,feature,[4])\n",
    "\n",
    "X = hrf_tools.apply_optimal_hrf_10hz(X,1)\n",
    "X_test = hrf_tools.apply_optimal_hrf_10hz(X_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2385, 521)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[:-500,:]\n",
    "Y_val = Y[:-500,:]\n",
    "X_train = X[-500:,:]\n",
    "Y_train = Y[-500:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 360)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[-500:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "time_steps = 10\n",
    "input_units = 521\n",
    "input_shape = (time_steps,input_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/api/preprocessing/timeseries/\n",
    "dataset_train = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_train, \n",
    "                                                     targets = Y_train, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)\n",
    "dataset_val = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_val, \n",
    "                                                     targets = Y_val, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)\n",
    "\n",
    "dataset_test = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_test, \n",
    "                                                     targets = Y_test, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### design a simple(?) lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_6 (Dropout)          (None, 10, 521)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 521)               2173612   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 521)               271962    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 521)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 521)               271962    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 521)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 360)               187920    \n",
      "=================================================================\n",
      "Total params: 2,905,456\n",
      "Trainable params: 2,905,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "#model.add(layers.Embedding(input_dim=1024, output_dim=1024))\n",
    "\n",
    "model.add(keras.Input(shape=input_shape)) #omit beacuse of bug\n",
    "model.add(layers.Dropout(0.1))\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(input_units))#,batch_input_shape=(batch_size, time_steps, input_units)))\n",
    "model.add(layers.Dense(521,activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(521,activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "#model.add(layers.Dense(360))\n",
    "\n",
    "#model.add(layers.UpSampling1D(size=10))\n",
    "#model.add(layers.Dense(17049))\n",
    "#model.add(layers.Dense(128))\n",
    "model.add(layers.Dense(360,activation='relu'))\n",
    "#model.add(layers.Dense(360))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(x_train,0), np.expand_dims(y_train,0)))\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_hrf, Y))\n",
    "\n",
    "# dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Buffer size to shuffle the dataset\n",
    "# # (TF data is designed to work with possibly infinite sequences,\n",
    "# # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# # it maintains a buffer in which it shuffles elements).\n",
    "# BUFFER_SIZE = 10000\n",
    "\n",
    "# dataset = (\n",
    "#     dataset\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(batch_size, drop_remainder=True)\n",
    "#     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#simple_rnn = layers.SimpleRNN(18, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_sequence_output, final_state = simple_rnn(np.expand_dims(x_train,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, restore_best_weights=True, patience=25)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
    "    optimizer=\"adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 0.6114 - val_loss: 0.5919\n",
      "Epoch 2/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6119 - val_loss: 0.5903\n",
      "Epoch 3/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6133 - val_loss: 0.5958\n",
      "Epoch 4/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6126 - val_loss: 0.5966\n",
      "Epoch 5/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6146 - val_loss: 0.5937\n",
      "Epoch 6/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6108 - val_loss: 0.5945\n",
      "Epoch 7/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6165 - val_loss: 0.5909\n",
      "Epoch 8/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6127 - val_loss: 0.5865\n",
      "Epoch 9/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6106 - val_loss: 0.5899\n",
      "Epoch 10/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6135 - val_loss: 0.5912\n",
      "Epoch 11/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6117 - val_loss: 0.5889\n",
      "Epoch 12/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6112 - val_loss: 0.5896\n",
      "Epoch 13/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6095 - val_loss: 0.5891\n",
      "Epoch 14/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6088 - val_loss: 0.5846\n",
      "Epoch 15/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6098 - val_loss: 0.5889\n",
      "Epoch 16/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6100 - val_loss: 0.5934\n",
      "Epoch 17/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6194 - val_loss: 0.6030\n",
      "Epoch 18/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6162 - val_loss: 0.5883\n",
      "Epoch 19/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6111 - val_loss: 0.5924\n",
      "Epoch 20/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6082 - val_loss: 0.5914\n",
      "Epoch 21/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6106 - val_loss: 0.5929\n",
      "Epoch 22/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6132 - val_loss: 0.5909\n",
      "Epoch 23/1000\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.6127 - val_loss: 0.5827\n",
      "Epoch 24/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6078 - val_loss: 0.5864\n",
      "Epoch 25/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6068 - val_loss: 0.5835\n",
      "Epoch 26/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6085 - val_loss: 0.5850\n",
      "Epoch 27/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6079 - val_loss: 0.5844\n",
      "Epoch 28/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6062 - val_loss: 0.5865\n",
      "Epoch 29/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6066 - val_loss: 0.5901\n",
      "Epoch 30/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6085 - val_loss: 0.5866\n",
      "Epoch 31/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6083 - val_loss: 0.5851\n",
      "Epoch 32/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6081 - val_loss: 0.5834\n",
      "Epoch 33/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6074 - val_loss: 0.5867\n",
      "Epoch 34/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6106 - val_loss: 0.5934\n",
      "Epoch 35/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6127 - val_loss: 0.5973\n",
      "Epoch 36/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6137 - val_loss: 0.5908\n",
      "Epoch 37/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6101 - val_loss: 0.5882\n",
      "Epoch 38/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6090 - val_loss: 0.5904\n",
      "Epoch 39/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6092 - val_loss: 0.5883\n",
      "Epoch 40/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6064 - val_loss: 0.5858\n",
      "Epoch 41/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6077 - val_loss: 0.5856\n",
      "Epoch 42/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6092 - val_loss: 0.5904\n",
      "Epoch 43/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6092 - val_loss: 0.5851\n",
      "Epoch 44/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6085 - val_loss: 0.5913\n",
      "Epoch 45/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6104 - val_loss: 0.5858\n",
      "Epoch 46/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6087 - val_loss: 0.5820\n",
      "Epoch 47/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6052 - val_loss: 0.5844\n",
      "Epoch 48/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6051 - val_loss: 0.5808\n",
      "Epoch 49/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6040 - val_loss: 0.5848\n",
      "Epoch 50/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6072 - val_loss: 0.5880\n",
      "Epoch 51/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6090 - val_loss: 0.5868\n",
      "Epoch 52/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6073 - val_loss: 0.5828\n",
      "Epoch 53/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6054 - val_loss: 0.5811\n",
      "Epoch 54/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6026 - val_loss: 0.5821\n",
      "Epoch 55/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6042 - val_loss: 0.5867\n",
      "Epoch 56/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6082 - val_loss: 0.5930\n",
      "Epoch 57/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6137 - val_loss: 0.5941\n",
      "Epoch 58/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6139 - val_loss: 0.5980\n",
      "Epoch 59/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6154 - val_loss: 0.5927\n",
      "Epoch 60/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6148 - val_loss: 0.5913\n",
      "Epoch 61/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6116 - val_loss: 0.5879\n",
      "Epoch 62/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6107 - val_loss: 0.5909\n",
      "Epoch 63/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6103 - val_loss: 0.5851\n",
      "Epoch 64/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6054 - val_loss: 0.5875\n",
      "Epoch 65/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6086 - val_loss: 0.5923\n",
      "Epoch 66/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6095 - val_loss: 0.5836\n",
      "Epoch 67/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6056 - val_loss: 0.5821\n",
      "Epoch 68/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6034 - val_loss: 0.5821\n",
      "Epoch 69/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6033 - val_loss: 0.5795\n",
      "Epoch 70/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6046 - val_loss: 0.5837\n",
      "Epoch 71/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6049 - val_loss: 0.5812\n",
      "Epoch 72/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6043 - val_loss: 0.5836\n",
      "Epoch 73/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6050 - val_loss: 0.5865\n",
      "Epoch 74/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6073 - val_loss: 0.5814\n",
      "Epoch 75/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6086 - val_loss: 0.5887\n",
      "Epoch 76/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6105 - val_loss: 0.5865\n",
      "Epoch 77/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6063 - val_loss: 0.5864\n",
      "Epoch 78/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6079 - val_loss: 0.5883\n",
      "Epoch 79/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6061 - val_loss: 0.5823\n",
      "Epoch 80/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6048 - val_loss: 0.5886\n",
      "Epoch 81/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6050 - val_loss: 0.5815\n",
      "Epoch 82/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6059 - val_loss: 0.5830\n",
      "Epoch 83/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6038 - val_loss: 0.5802\n",
      "Epoch 84/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6043 - val_loss: 0.5786\n",
      "Epoch 85/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6009 - val_loss: 0.5807\n",
      "Epoch 86/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6029 - val_loss: 0.5915\n",
      "Epoch 87/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6040 - val_loss: 0.5855\n",
      "Epoch 88/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6073 - val_loss: 0.5883\n",
      "Epoch 89/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6067 - val_loss: 0.5848\n",
      "Epoch 90/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6065 - val_loss: 0.5825\n",
      "Epoch 91/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6047 - val_loss: 0.5779\n",
      "Epoch 92/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.6041 - val_loss: 0.5807\n",
      "Epoch 93/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6053 - val_loss: 0.5848\n",
      "Epoch 94/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6079 - val_loss: 0.5833\n",
      "Epoch 95/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6048 - val_loss: 0.5841\n",
      "Epoch 96/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6032 - val_loss: 0.5796\n",
      "Epoch 97/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6016 - val_loss: 0.5803\n",
      "Epoch 98/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6015 - val_loss: 0.5812\n",
      "Epoch 99/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6035 - val_loss: 0.5890\n",
      "Epoch 100/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6072 - val_loss: 0.5864\n",
      "Epoch 101/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6071 - val_loss: 0.5840\n",
      "Epoch 102/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6040 - val_loss: 0.5815\n",
      "Epoch 103/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6049 - val_loss: 0.5850\n",
      "Epoch 104/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6050 - val_loss: 0.5838\n",
      "Epoch 105/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6046 - val_loss: 0.5858\n",
      "Epoch 106/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6027 - val_loss: 0.5835\n",
      "Epoch 107/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6046 - val_loss: 0.5793\n",
      "Epoch 108/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6025 - val_loss: 0.5820\n",
      "Epoch 109/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6030 - val_loss: 0.5806\n",
      "Epoch 110/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6011 - val_loss: 0.5756\n",
      "Epoch 111/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5985 - val_loss: 0.5759\n",
      "Epoch 112/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5986 - val_loss: 0.5789\n",
      "Epoch 113/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6015 - val_loss: 0.5819\n",
      "Epoch 114/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6030 - val_loss: 0.5744\n",
      "Epoch 115/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5998 - val_loss: 0.5780\n",
      "Epoch 116/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6009 - val_loss: 0.5764\n",
      "Epoch 117/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6021 - val_loss: 0.5868\n",
      "Epoch 118/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6036 - val_loss: 0.5815\n",
      "Epoch 119/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6050 - val_loss: 0.5820\n",
      "Epoch 120/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6032 - val_loss: 0.5832\n",
      "Epoch 121/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6011 - val_loss: 0.5782\n",
      "Epoch 122/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6013 - val_loss: 0.5796\n",
      "Epoch 123/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6010 - val_loss: 0.5787\n",
      "Epoch 124/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6021 - val_loss: 0.5843\n",
      "Epoch 125/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6014 - val_loss: 0.5810\n",
      "Epoch 126/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6006 - val_loss: 0.5770\n",
      "Epoch 127/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5989 - val_loss: 0.5744\n",
      "Epoch 128/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5986 - val_loss: 0.5773\n",
      "Epoch 129/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5983 - val_loss: 0.5803\n",
      "Epoch 130/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6015 - val_loss: 0.5799\n",
      "Epoch 131/1000\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6032 - val_loss: 0.5797\n",
      "Epoch 132/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6015 - val_loss: 0.5830\n",
      "Epoch 133/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6039 - val_loss: 0.5827\n",
      "Epoch 134/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6021 - val_loss: 0.5797\n",
      "Epoch 135/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6008 - val_loss: 0.5776\n",
      "Epoch 136/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6009 - val_loss: 0.5792\n",
      "Epoch 137/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5999 - val_loss: 0.5783\n",
      "Epoch 138/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6025 - val_loss: 0.5768\n",
      "Epoch 139/1000\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6021 - val_loss: 0.5781\n",
      "Epoch 140/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6025 - val_loss: 0.5785\n",
      "Epoch 141/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6035 - val_loss: 0.5849\n",
      "Epoch 142/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6004 - val_loss: 0.5834\n",
      "Epoch 143/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6038 - val_loss: 0.5793\n",
      "Epoch 144/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6026 - val_loss: 0.5805\n",
      "Epoch 145/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6021 - val_loss: 0.5839\n",
      "Epoch 146/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6011 - val_loss: 0.5778\n",
      "Epoch 147/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6003 - val_loss: 0.5780\n",
      "Epoch 148/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5998 - val_loss: 0.5800\n",
      "Epoch 149/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6010 - val_loss: 0.5788\n",
      "Epoch 150/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6034 - val_loss: 0.5814\n",
      "Epoch 151/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5989 - val_loss: 0.5834\n",
      "Epoch 152/1000\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6030 - val_loss: 0.5828\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00152: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,epochs = 1000, validation_data=dataset_val, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../outputs/mmp_earlystop_00511.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../outputs/mmp_earlystop_00511.pb/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../outputs/mmp_earlystop_00511.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../outputs/model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.keras.preprocessing.timeseries_dataset_from_array(data = X_test, \n",
    "                                                     targets = Y_test, \n",
    "                                                    sequence_length=time_steps,\n",
    "                                                     batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 12ms/step - loss: 1.0640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0639604330062866"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 360)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predicted = model.predict(dataset_test)\n",
    "# it is 360 long not 380, so it must be only cortex\n",
    "Y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import npp\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_test[9:,:], Y_predicted,multioutput='raw_values')\n",
    "r = npp.mcorr(Y_test[9:,:], Y_predicted)\n",
    "r = np.append(r, np.zeros(20))\n",
    "r2 = np.append(r2, np.zeros(20))\n",
    "\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = np.append(r2, np.zeros(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import nilearn.plotting as plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import hcp_utils as hcp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380,)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.60541544 -0.58934379 -0.55266707 -0.50050411 -0.43722121 -0.43558113\n",
      " -0.41723345 -0.41123071 -0.40820145 -0.40738757 -0.400753   -0.3961009\n",
      " -0.3959338  -0.39348458 -0.39226126 -0.38949863 -0.38928398 -0.38773321\n",
      " -0.38592725 -0.3858073  -0.38297276 -0.37757682 -0.37720798 -0.3770676\n",
      " -0.36981633 -0.36366198 -0.3629101  -0.36234956 -0.35585319 -0.35490243\n",
      " -0.35271716 -0.35221839 -0.35212463 -0.3504849  -0.34967885 -0.34889434\n",
      " -0.34572595 -0.34560158 -0.34537841 -0.34528762 -0.33808161 -0.33682879\n",
      " -0.33605215 -0.3334054  -0.33048122 -0.32553797 -0.3253692  -0.32113236\n",
      " -0.32099961 -0.31846672 -0.3184623  -0.31648036 -0.31632305 -0.31533863\n",
      " -0.31523559 -0.31514684 -0.31408676 -0.31331144 -0.30981342 -0.30578311\n",
      " -0.29900258 -0.29684165 -0.29563076 -0.2955406  -0.29374022 -0.29264572\n",
      " -0.28788534 -0.28646566 -0.28299045 -0.28288196 -0.28215919 -0.28147888\n",
      " -0.28112779 -0.28049498 -0.28011843 -0.2764798  -0.27533344 -0.27441443\n",
      " -0.27425846 -0.27261367 -0.27164958 -0.26756589 -0.26630498 -0.266021\n",
      " -0.26510744 -0.26448562 -0.26053517 -0.25926944 -0.25459568 -0.25458344\n",
      " -0.25330525 -0.25192683 -0.25067802 -0.24959725 -0.24828424 -0.24809929\n",
      " -0.24784285 -0.24664648 -0.24626951 -0.2456209  -0.24525675 -0.24430683\n",
      " -0.24394069 -0.24208827 -0.23814138 -0.23690705 -0.23201227 -0.23105714\n",
      " -0.23002298 -0.22779789 -0.22751941 -0.22546198 -0.22529994 -0.22468818\n",
      " -0.22415225 -0.22371477 -0.22290212 -0.2227214  -0.22259221 -0.22239848\n",
      " -0.22209303 -0.22169463 -0.22087642 -0.22056415 -0.22045297 -0.21992405\n",
      " -0.21983658 -0.21837977 -0.2152579  -0.21507545 -0.21339179 -0.20980872\n",
      " -0.20936245 -0.20897883 -0.2083032  -0.20819493 -0.2075784  -0.20473159\n",
      " -0.20229926 -0.20147005 -0.20110946 -0.19895928 -0.19696136 -0.19554964\n",
      " -0.19535322 -0.19500985 -0.19287816 -0.19012931 -0.18833762 -0.18739984\n",
      " -0.18648376 -0.18617706 -0.18419132 -0.18274587 -0.17871051 -0.17830754\n",
      " -0.17236742 -0.16859156 -0.168114   -0.16728216 -0.16123816 -0.15646628\n",
      " -0.15398219 -0.15294064 -0.15024279 -0.14832965 -0.14707636 -0.14237826\n",
      " -0.14237269 -0.14190071 -0.13332085 -0.12310095 -0.11315969 -0.10315583\n",
      " -0.09123321 -0.07950798 -0.07441602 -0.06935119 -0.06396981 -0.04379093\n",
      "  0.        ]\n",
      "[-0.55945029 -0.55880815 -0.54905293 -0.49775337 -0.47669904 -0.46128466\n",
      " -0.43816289 -0.4202706  -0.40917171 -0.40567174 -0.40190566 -0.40146646\n",
      " -0.39715329 -0.39177203 -0.38841676 -0.3880555  -0.37339268 -0.37181696\n",
      " -0.37062136 -0.36912917 -0.36716507 -0.36678757 -0.36522406 -0.36455226\n",
      " -0.3645352  -0.36261772 -0.36150803 -0.35907707 -0.35812744 -0.35794989\n",
      " -0.35728753 -0.3553511  -0.35518348 -0.34967793 -0.34825878 -0.34614136\n",
      " -0.34524568 -0.34484369 -0.3446023  -0.34344892 -0.34230026 -0.34022087\n",
      " -0.33951706 -0.33894347 -0.33330858 -0.3315204  -0.32951044 -0.3253625\n",
      " -0.32515144 -0.32203778 -0.32151362 -0.32073383 -0.32051634 -0.3179068\n",
      " -0.31600469 -0.31522594 -0.3131561  -0.31310559 -0.31268494 -0.31131803\n",
      " -0.30610742 -0.30556694 -0.30526626 -0.30453188 -0.30417992 -0.30345961\n",
      " -0.30250667 -0.30175175 -0.30169267 -0.30160347 -0.30097106 -0.30025977\n",
      " -0.29943692 -0.29867038 -0.29586039 -0.29534613 -0.29391559 -0.29377133\n",
      " -0.29227961 -0.29028092 -0.28962095 -0.28911068 -0.28825484 -0.28629378\n",
      " -0.28481228 -0.28427864 -0.28097091 -0.28081718 -0.27601337 -0.27592935\n",
      " -0.27544329 -0.27212474 -0.27183775 -0.27148166 -0.27117493 -0.2704555\n",
      " -0.26742637 -0.26733331 -0.26709242 -0.26700627 -0.26407668 -0.26404111\n",
      " -0.26370325 -0.26344715 -0.26262666 -0.26168474 -0.2583989  -0.2569664\n",
      " -0.2560207  -0.25301061 -0.25008887 -0.24905177 -0.24868125 -0.24849307\n",
      " -0.24743915 -0.24597609 -0.24424316 -0.24358512 -0.24325206 -0.24044124\n",
      " -0.23811658 -0.23671042 -0.23572949 -0.23496915 -0.22994638 -0.22983179\n",
      " -0.22971015 -0.22720921 -0.2262288  -0.22514176 -0.22442138 -0.22416315\n",
      " -0.22013189 -0.21851229 -0.21733102 -0.21629085 -0.2159219  -0.21404769\n",
      " -0.21389092 -0.21334278 -0.21275692 -0.21114278 -0.21078715 -0.20905371\n",
      " -0.20733148 -0.20516387 -0.20494661 -0.19777556 -0.1962056  -0.19617927\n",
      " -0.19447262 -0.19247059 -0.19050018 -0.19001834 -0.18703874 -0.18662504\n",
      " -0.18544341 -0.1846139  -0.18430441 -0.18307237 -0.18128571 -0.17719035\n",
      " -0.16303216 -0.15893027 -0.15864034 -0.1543506  -0.14929994 -0.14923469\n",
      " -0.14472903 -0.14341537 -0.1426456  -0.13916356 -0.12174373 -0.12085052\n",
      " -0.11934996 -0.11022301 -0.09726913 -0.09524254 -0.0945237  -0.05827374\n",
      "  0.        ]\n",
      "[-0.60541544 -0.58934379 -0.55266707 -0.50050411 -0.43722121 -0.43558113\n",
      " -0.41723345 -0.41123071 -0.40820145 -0.40738757 -0.400753   -0.3961009\n",
      " -0.3959338  -0.39348458 -0.39226126 -0.38949863 -0.38928398 -0.38773321\n",
      " -0.38592725 -0.3858073  -0.38297276 -0.37757682 -0.37720798 -0.3770676\n",
      " -0.36981633 -0.36366198 -0.3629101  -0.36234956 -0.35585319 -0.35490243\n",
      " -0.35271716 -0.35221839 -0.35212463 -0.3504849  -0.34967885 -0.34889434\n",
      " -0.34572595 -0.34560158 -0.34537841 -0.34528762 -0.33808161 -0.33682879\n",
      " -0.33605215 -0.3334054  -0.33048122 -0.32553797 -0.3253692  -0.32113236\n",
      " -0.32099961 -0.31846672 -0.3184623  -0.31648036 -0.31632305 -0.31533863\n",
      " -0.31523559 -0.31514684 -0.31408676 -0.31331144 -0.30981342 -0.30578311\n",
      " -0.29900258 -0.29684165 -0.29563076 -0.2955406  -0.29374022 -0.29264572\n",
      " -0.28788534 -0.28646566 -0.28299045 -0.28288196 -0.28215919 -0.28147888\n",
      " -0.28112779 -0.28049498 -0.28011843 -0.2764798  -0.27533344 -0.27441443\n",
      " -0.27425846 -0.27261367 -0.27164958 -0.26756589 -0.26630498 -0.266021\n",
      " -0.26510744 -0.26448562 -0.26053517 -0.25926944 -0.25459568 -0.25458344\n",
      " -0.25330525 -0.25192683 -0.25067802 -0.24959725 -0.24828424 -0.24809929\n",
      " -0.24784285 -0.24664648 -0.24626951 -0.2456209  -0.24525675 -0.24430683\n",
      " -0.24394069 -0.24208827 -0.23814138 -0.23690705 -0.23201227 -0.23105714\n",
      " -0.23002298 -0.22779789 -0.22751941 -0.22546198 -0.22529994 -0.22468818\n",
      " -0.22415225 -0.22371477 -0.22290212 -0.2227214  -0.22259221 -0.22239848\n",
      " -0.22209303 -0.22169463 -0.22087642 -0.22056415 -0.22045297 -0.21992405\n",
      " -0.21983658 -0.21837977 -0.2152579  -0.21507545 -0.21339179 -0.20980872\n",
      " -0.20936245 -0.20897883 -0.2083032  -0.20819493 -0.2075784  -0.20473159\n",
      " -0.20229926 -0.20147005 -0.20110946 -0.19895928 -0.19696136 -0.19554964\n",
      " -0.19535322 -0.19500985 -0.19287816 -0.19012931 -0.18833762 -0.18739984\n",
      " -0.18648376 -0.18617706 -0.18419132 -0.18274587 -0.17871051 -0.17830754\n",
      " -0.17236742 -0.16859156 -0.168114   -0.16728216 -0.16123816 -0.15646628\n",
      " -0.15398219 -0.15294064 -0.15024279 -0.14832965 -0.14707636 -0.14237826\n",
      " -0.14237269 -0.14190071 -0.13332085 -0.12310095 -0.11315969 -0.10315583\n",
      " -0.09123321 -0.07950798 -0.07441602 -0.06935119 -0.06396981 -0.04379093\n",
      "  0.        ]\n",
      "[-0.55945029 -0.55880815 -0.54905293 -0.49775337 -0.47669904 -0.46128466\n",
      " -0.43816289 -0.4202706  -0.40917171 -0.40567174 -0.40190566 -0.40146646\n",
      " -0.39715329 -0.39177203 -0.38841676 -0.3880555  -0.37339268 -0.37181696\n",
      " -0.37062136 -0.36912917 -0.36716507 -0.36678757 -0.36522406 -0.36455226\n",
      " -0.3645352  -0.36261772 -0.36150803 -0.35907707 -0.35812744 -0.35794989\n",
      " -0.35728753 -0.3553511  -0.35518348 -0.34967793 -0.34825878 -0.34614136\n",
      " -0.34524568 -0.34484369 -0.3446023  -0.34344892 -0.34230026 -0.34022087\n",
      " -0.33951706 -0.33894347 -0.33330858 -0.3315204  -0.32951044 -0.3253625\n",
      " -0.32515144 -0.32203778 -0.32151362 -0.32073383 -0.32051634 -0.3179068\n",
      " -0.31600469 -0.31522594 -0.3131561  -0.31310559 -0.31268494 -0.31131803\n",
      " -0.30610742 -0.30556694 -0.30526626 -0.30453188 -0.30417992 -0.30345961\n",
      " -0.30250667 -0.30175175 -0.30169267 -0.30160347 -0.30097106 -0.30025977\n",
      " -0.29943692 -0.29867038 -0.29586039 -0.29534613 -0.29391559 -0.29377133\n",
      " -0.29227961 -0.29028092 -0.28962095 -0.28911068 -0.28825484 -0.28629378\n",
      " -0.28481228 -0.28427864 -0.28097091 -0.28081718 -0.27601337 -0.27592935\n",
      " -0.27544329 -0.27212474 -0.27183775 -0.27148166 -0.27117493 -0.2704555\n",
      " -0.26742637 -0.26733331 -0.26709242 -0.26700627 -0.26407668 -0.26404111\n",
      " -0.26370325 -0.26344715 -0.26262666 -0.26168474 -0.2583989  -0.2569664\n",
      " -0.2560207  -0.25301061 -0.25008887 -0.24905177 -0.24868125 -0.24849307\n",
      " -0.24743915 -0.24597609 -0.24424316 -0.24358512 -0.24325206 -0.24044124\n",
      " -0.23811658 -0.23671042 -0.23572949 -0.23496915 -0.22994638 -0.22983179\n",
      " -0.22971015 -0.22720921 -0.2262288  -0.22514176 -0.22442138 -0.22416315\n",
      " -0.22013189 -0.21851229 -0.21733102 -0.21629085 -0.2159219  -0.21404769\n",
      " -0.21389092 -0.21334278 -0.21275692 -0.21114278 -0.21078715 -0.20905371\n",
      " -0.20733148 -0.20516387 -0.20494661 -0.19777556 -0.1962056  -0.19617927\n",
      " -0.19447262 -0.19247059 -0.19050018 -0.19001834 -0.18703874 -0.18662504\n",
      " -0.18544341 -0.1846139  -0.18430441 -0.18307237 -0.18128571 -0.17719035\n",
      " -0.16303216 -0.15893027 -0.15864034 -0.1543506  -0.14929994 -0.14923469\n",
      " -0.14472903 -0.14341537 -0.1426456  -0.13916356 -0.12174373 -0.12085052\n",
      " -0.11934996 -0.11022301 -0.09726913 -0.09524254 -0.0945237  -0.05827374\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "from analysis import plot_32k_results\n",
    "plot_32k_results(hcp.unparcellate(r2, hcp.mmp),'r2',100610,'as_scores','HCP_MMP_LSTM10_e511_r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare to encoding model simple ridgecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import simple_ridgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mean,corr_mean,weights_mean = simple_ridgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=100610\n",
    "feature='as_scores'\n",
    "n_movies=[1,2,3,4]\n",
    "X,Y = load_data_HCP_MMP(subject,feature,n_movies)\n",
    "\n",
    "X = hrf_tools.apply_optimal_hrf_10hz(X,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mean,corr_mean,weights_mean = simple_ridgeCV(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07379453 -0.0594298  -0.05796366 -0.05428356 -0.05242199 -0.05037978\n",
      " -0.049447   -0.03912499 -0.0343299  -0.02907795 -0.02699877 -0.02612969\n",
      " -0.02077062 -0.01689491 -0.01671508 -0.01365151 -0.01075032 -0.00795606\n",
      " -0.00239166  0.          0.00158645  0.00167508  0.00246106  0.00385166\n",
      "  0.01373988  0.01395701  0.01607101  0.01708734  0.02032552  0.02125598\n",
      "  0.02403139  0.02484643  0.02609002  0.03300244  0.03415697  0.03501747\n",
      "  0.03591688  0.03648897  0.03809661  0.03885547  0.04095772  0.04195697\n",
      "  0.04272116  0.04704868  0.04987922  0.0502834   0.0503497   0.05076438\n",
      "  0.05140057  0.05612188  0.05691978  0.05892088  0.0591216   0.05988332\n",
      "  0.06182481  0.06386767  0.06407604  0.06750781  0.06954963  0.06985272\n",
      "  0.07025336  0.07210307  0.07325805  0.07348103  0.07398143  0.07451719\n",
      "  0.07876466  0.07901132  0.07923717  0.08304534  0.08666907  0.08770605\n",
      "  0.08849491  0.08945132  0.09123561  0.09129707  0.09207967  0.09313536\n",
      "  0.09456293  0.09523509  0.09714286  0.1040952   0.10489599  0.10742577\n",
      "  0.10768127  0.10792847  0.10917271  0.11021492  0.111506    0.11189704\n",
      "  0.12023438  0.12112215  0.12263581  0.12518411  0.12702144  0.13288727\n",
      "  0.13674391  0.13723105  0.13851302  0.14099943  0.14100255  0.14407493\n",
      "  0.15193582  0.15913041  0.16007197  0.16032086  0.16369233  0.16955414\n",
      "  0.16980501  0.17201801  0.17908914  0.18033448  0.18225883  0.18359271\n",
      "  0.18411359  0.18448369  0.1896611   0.1947015   0.19721417  0.19842578\n",
      "  0.19870607  0.19911846  0.20368225  0.2046203   0.20848051  0.21101375\n",
      "  0.21863073  0.22049284  0.22113234  0.22140232  0.22232517  0.22278271\n",
      "  0.22375085  0.22786036  0.23162649  0.2328562   0.23527134  0.23581289\n",
      "  0.2358624   0.24087523  0.24303138  0.26143891  0.26171715  0.26267416\n",
      "  0.26471861  0.26721925  0.26908275  0.27182079  0.2729347   0.27345901\n",
      "  0.27414741  0.27759807  0.28279082  0.28340433  0.30199155  0.30586082\n",
      "  0.30852909  0.31117376  0.31608843  0.31955367  0.32215398  0.32400307\n",
      "  0.32912657  0.33325175  0.34013759  0.34311791  0.34484873  0.34806745\n",
      "  0.35838826  0.36776601  0.40966665  0.40986879  0.42648739  0.4384191\n",
      "  0.43940848  0.4446686   0.45725145  0.47168378  0.5549295   0.56295606\n",
      "  0.56694389]\n",
      "[-0.06805973 -0.05920345 -0.05853403 -0.05825121 -0.0571294  -0.05580282\n",
      " -0.05048206 -0.04083124 -0.03311271 -0.03216592 -0.02837485 -0.02786814\n",
      " -0.02758667 -0.02512697 -0.02390487 -0.01495839 -0.01321485 -0.01283717\n",
      " -0.01167146 -0.00881917 -0.00862827 -0.00526908 -0.00459583  0.\n",
      "  0.00333527  0.00570134  0.01369771  0.01420431  0.01460383  0.01469947\n",
      "  0.01534706  0.01535601  0.01590455  0.0169472   0.01756077  0.01807911\n",
      "  0.01839772  0.02054591  0.02228928  0.02433558  0.02461011  0.02958666\n",
      "  0.03252986  0.03254239  0.03463131  0.03873015  0.04026645  0.04199637\n",
      "  0.04475959  0.04590699  0.05030924  0.05110383  0.05136626  0.05228426\n",
      "  0.05324312  0.05475012  0.05833768  0.05965049  0.06034905  0.06035914\n",
      "  0.06063154  0.06583486  0.06650693  0.06650887  0.06825021  0.06894096\n",
      "  0.07063829  0.07111586  0.07460183  0.07673679  0.07916134  0.07981214\n",
      "  0.08149552  0.08207693  0.08487568  0.08629688  0.08919363  0.09109812\n",
      "  0.0960107   0.09712329  0.09860567  0.09894346  0.10012214  0.10906638\n",
      "  0.10968971  0.11297838  0.11603301  0.12208325  0.12298591  0.12435825\n",
      "  0.12556812  0.13091858  0.13193714  0.1346021   0.13627306  0.13891562\n",
      "  0.14328464  0.14729056  0.1496358   0.15847432  0.16216543  0.1652603\n",
      "  0.16843497  0.16873215  0.16982432  0.16999181  0.17226532  0.17269786\n",
      "  0.17454     0.17490646  0.17828742  0.18096261  0.18115746  0.18658516\n",
      "  0.19043446  0.19360886  0.19607842  0.19996443  0.2008355   0.20126891\n",
      "  0.20164746  0.20596156  0.20860828  0.21900602  0.22061226  0.22786439\n",
      "  0.22835768  0.2307135   0.23142634  0.23515351  0.23676803  0.23947061\n",
      "  0.24323183  0.24550823  0.24569803  0.24897642  0.24949033  0.25388385\n",
      "  0.25981382  0.26357617  0.26396849  0.26703717  0.2711941   0.27933687\n",
      "  0.27941497  0.28129025  0.28707061  0.28832736  0.28918684  0.28949215\n",
      "  0.29227737  0.29580208  0.29621509  0.30000604  0.3031903   0.31968456\n",
      "  0.32882954  0.32980934  0.33084782  0.33171941  0.3373129   0.33908721\n",
      "  0.34256562  0.35068714  0.35123876  0.35186328  0.37643533  0.3896937\n",
      "  0.39089134  0.40805966  0.41446568  0.42264324  0.42861967  0.44007076\n",
      "  0.47917799  0.48642038  0.49196829  0.51157161  0.51844023  0.52603107\n",
      "  0.56409036]\n",
      "[-0.07379453 -0.0594298  -0.05796366 -0.05428356 -0.05242199 -0.05037978\n",
      " -0.049447   -0.03912499 -0.0343299  -0.02907795 -0.02699877 -0.02612969\n",
      " -0.02077062 -0.01689491 -0.01671508 -0.01365151 -0.01075032 -0.00795606\n",
      " -0.00239166  0.          0.00158645  0.00167508  0.00246106  0.00385166\n",
      "  0.01373988  0.01395701  0.01607101  0.01708734  0.02032552  0.02125598\n",
      "  0.02403139  0.02484643  0.02609002  0.03300244  0.03415697  0.03501747\n",
      "  0.03591688  0.03648897  0.03809661  0.03885547  0.04095772  0.04195697\n",
      "  0.04272116  0.04704868  0.04987922  0.0502834   0.0503497   0.05076438\n",
      "  0.05140057  0.05612188  0.05691978  0.05892088  0.0591216   0.05988332\n",
      "  0.06182481  0.06386767  0.06407604  0.06750781  0.06954963  0.06985272\n",
      "  0.07025336  0.07210307  0.07325805  0.07348103  0.07398143  0.07451719\n",
      "  0.07876466  0.07901132  0.07923717  0.08304534  0.08666907  0.08770605\n",
      "  0.08849491  0.08945132  0.09123561  0.09129707  0.09207967  0.09313536\n",
      "  0.09456293  0.09523509  0.09714286  0.1040952   0.10489599  0.10742577\n",
      "  0.10768127  0.10792847  0.10917271  0.11021492  0.111506    0.11189704\n",
      "  0.12023438  0.12112215  0.12263581  0.12518411  0.12702144  0.13288727\n",
      "  0.13674391  0.13723105  0.13851302  0.14099943  0.14100255  0.14407493\n",
      "  0.15193582  0.15913041  0.16007197  0.16032086  0.16369233  0.16955414\n",
      "  0.16980501  0.17201801  0.17908914  0.18033448  0.18225883  0.18359271\n",
      "  0.18411359  0.18448369  0.1896611   0.1947015   0.19721417  0.19842578\n",
      "  0.19870607  0.19911846  0.20368225  0.2046203   0.20848051  0.21101375\n",
      "  0.21863073  0.22049284  0.22113234  0.22140232  0.22232517  0.22278271\n",
      "  0.22375085  0.22786036  0.23162649  0.2328562   0.23527134  0.23581289\n",
      "  0.2358624   0.24087523  0.24303138  0.26143891  0.26171715  0.26267416\n",
      "  0.26471861  0.26721925  0.26908275  0.27182079  0.2729347   0.27345901\n",
      "  0.27414741  0.27759807  0.28279082  0.28340433  0.30199155  0.30586082\n",
      "  0.30852909  0.31117376  0.31608843  0.31955367  0.32215398  0.32400307\n",
      "  0.32912657  0.33325175  0.34013759  0.34311791  0.34484873  0.34806745\n",
      "  0.35838826  0.36776601  0.40966665  0.40986879  0.42648739  0.4384191\n",
      "  0.43940848  0.4446686   0.45725145  0.47168378  0.5549295   0.56295606\n",
      "  0.56694389]\n",
      "[-0.06805973 -0.05920345 -0.05853403 -0.05825121 -0.0571294  -0.05580282\n",
      " -0.05048206 -0.04083124 -0.03311271 -0.03216592 -0.02837485 -0.02786814\n",
      " -0.02758667 -0.02512697 -0.02390487 -0.01495839 -0.01321485 -0.01283717\n",
      " -0.01167146 -0.00881917 -0.00862827 -0.00526908 -0.00459583  0.\n",
      "  0.00333527  0.00570134  0.01369771  0.01420431  0.01460383  0.01469947\n",
      "  0.01534706  0.01535601  0.01590455  0.0169472   0.01756077  0.01807911\n",
      "  0.01839772  0.02054591  0.02228928  0.02433558  0.02461011  0.02958666\n",
      "  0.03252986  0.03254239  0.03463131  0.03873015  0.04026645  0.04199637\n",
      "  0.04475959  0.04590699  0.05030924  0.05110383  0.05136626  0.05228426\n",
      "  0.05324312  0.05475012  0.05833768  0.05965049  0.06034905  0.06035914\n",
      "  0.06063154  0.06583486  0.06650693  0.06650887  0.06825021  0.06894096\n",
      "  0.07063829  0.07111586  0.07460183  0.07673679  0.07916134  0.07981214\n",
      "  0.08149552  0.08207693  0.08487568  0.08629688  0.08919363  0.09109812\n",
      "  0.0960107   0.09712329  0.09860567  0.09894346  0.10012214  0.10906638\n",
      "  0.10968971  0.11297838  0.11603301  0.12208325  0.12298591  0.12435825\n",
      "  0.12556812  0.13091858  0.13193714  0.1346021   0.13627306  0.13891562\n",
      "  0.14328464  0.14729056  0.1496358   0.15847432  0.16216543  0.1652603\n",
      "  0.16843497  0.16873215  0.16982432  0.16999181  0.17226532  0.17269786\n",
      "  0.17454     0.17490646  0.17828742  0.18096261  0.18115746  0.18658516\n",
      "  0.19043446  0.19360886  0.19607842  0.19996443  0.2008355   0.20126891\n",
      "  0.20164746  0.20596156  0.20860828  0.21900602  0.22061226  0.22786439\n",
      "  0.22835768  0.2307135   0.23142634  0.23515351  0.23676803  0.23947061\n",
      "  0.24323183  0.24550823  0.24569803  0.24897642  0.24949033  0.25388385\n",
      "  0.25981382  0.26357617  0.26396849  0.26703717  0.2711941   0.27933687\n",
      "  0.27941497  0.28129025  0.28707061  0.28832736  0.28918684  0.28949215\n",
      "  0.29227737  0.29580208  0.29621509  0.30000604  0.3031903   0.31968456\n",
      "  0.32882954  0.32980934  0.33084782  0.33171941  0.3373129   0.33908721\n",
      "  0.34256562  0.35068714  0.35123876  0.35186328  0.37643533  0.3896937\n",
      "  0.39089134  0.40805966  0.41446568  0.42264324  0.42861967  0.44007076\n",
      "  0.47917799  0.48642038  0.49196829  0.51157161  0.51844023  0.52603107\n",
      "  0.56409036]\n"
     ]
    }
   ],
   "source": [
    "corr_mean = np.append(corr_mean, np.zeros(20))\n",
    "plot_32k_results(hcp.unparcellate(corr_mean, hcp.mmp),'r',100610,'as_scores','HCP_MMP_ridgecv_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.84086601e-01 -3.58073785e-01 -3.39105902e-01 -3.29342852e-01\n",
      " -2.92308640e-01 -2.65493633e-01 -2.18180799e-01 -2.14697223e-01\n",
      " -2.09658447e-01 -2.01116373e-01 -1.97986011e-01 -1.97691755e-01\n",
      " -1.95378931e-01 -1.86271333e-01 -1.84490837e-01 -1.80938245e-01\n",
      " -1.79901183e-01 -1.79586407e-01 -1.75294187e-01 -1.71682008e-01\n",
      " -1.68478646e-01 -1.63028866e-01 -1.62737529e-01 -1.61268650e-01\n",
      " -1.59853997e-01 -1.58893601e-01 -1.57880894e-01 -1.54656457e-01\n",
      " -1.54417025e-01 -1.53443869e-01 -1.52695476e-01 -1.49738847e-01\n",
      " -1.49032309e-01 -1.46248910e-01 -1.45070043e-01 -1.43083167e-01\n",
      " -1.42728038e-01 -1.41936551e-01 -1.39327337e-01 -1.39053068e-01\n",
      " -1.38116434e-01 -1.38067119e-01 -1.37482099e-01 -1.34571376e-01\n",
      " -1.34104264e-01 -1.33612255e-01 -1.32643837e-01 -1.32297484e-01\n",
      " -1.28544002e-01 -1.27321714e-01 -1.21209504e-01 -1.18182741e-01\n",
      " -1.16651178e-01 -1.15489435e-01 -1.13360337e-01 -1.12175584e-01\n",
      " -1.08615063e-01 -1.08603649e-01 -1.08396269e-01 -1.05565430e-01\n",
      " -1.05540455e-01 -1.04590276e-01 -1.00677985e-01 -1.00452799e-01\n",
      " -9.96140763e-02 -9.87432797e-02 -9.60269398e-02 -9.41759075e-02\n",
      " -9.36741638e-02 -9.36693363e-02 -9.22796972e-02 -8.84373863e-02\n",
      " -8.16729036e-02 -7.70137593e-02 -7.60931231e-02 -7.25225968e-02\n",
      " -7.09841192e-02 -7.08028550e-02 -7.06117046e-02 -6.80981983e-02\n",
      " -6.76965891e-02 -6.74377909e-02 -6.28909220e-02 -6.00639135e-02\n",
      " -5.88449881e-02 -5.74303297e-02 -5.44736662e-02 -5.39878268e-02\n",
      " -5.27059462e-02 -4.81755182e-02 -4.76692953e-02 -4.72815812e-02\n",
      " -4.55952094e-02 -4.45021701e-02 -4.33140468e-02 -4.23604168e-02\n",
      " -4.15816762e-02 -4.13214554e-02 -3.94244655e-02 -3.70974050e-02\n",
      " -3.67809042e-02 -3.35576005e-02 -3.26816287e-02 -3.05128919e-02\n",
      " -2.87716435e-02 -2.37394007e-02 -2.33540823e-02 -2.31419137e-02\n",
      " -2.01849969e-02 -1.99359916e-02 -1.93672599e-02 -1.66682880e-02\n",
      " -1.53378787e-02 -1.36384049e-02 -1.30193846e-02 -1.26305957e-02\n",
      " -1.11995831e-02 -1.06113861e-02 -7.46354961e-03 -7.13735295e-03\n",
      " -6.04832947e-03 -4.64401407e-03 -1.52862488e-03 -1.30584008e-03\n",
      " -7.25052438e-04  0.00000000e+00  1.77881380e-05  1.30275454e-03\n",
      "  4.15947917e-03  5.05608114e-03  6.18733741e-03  8.69503224e-03\n",
      "  8.72325343e-03  9.85055037e-03  1.29618820e-02  1.58921846e-02\n",
      "  1.60217281e-02  1.85915652e-02  1.87282623e-02  2.37546509e-02\n",
      "  2.48281607e-02  2.53840795e-02  2.73791805e-02  2.88349460e-02\n",
      "  3.00381799e-02  3.13825838e-02  3.44201675e-02  3.65788097e-02\n",
      "  4.12902355e-02  4.24255984e-02  4.46517986e-02  4.86572504e-02\n",
      "  4.96038244e-02  5.52110729e-02  5.62090180e-02  6.29809763e-02\n",
      "  7.13145445e-02  7.19307925e-02  7.28165721e-02  8.50919933e-02\n",
      "  8.55364385e-02  9.41112903e-02  1.06107077e-01  1.10838380e-01\n",
      "  1.45277042e-01  1.49164737e-01  1.63787780e-01  1.69948533e-01\n",
      "  1.80243332e-01  2.29191487e-01  2.67136542e-01  3.04970864e-01\n",
      "  3.06973438e-01  3.07030299e-01  3.20667905e-01  3.26683232e-01\n",
      "  3.51019992e-01  3.94657259e-01  5.22011098e-01  5.44672978e-01\n",
      "  5.82587975e-01]\n",
      "[-0.4085807  -0.35305706 -0.34218079 -0.3331316  -0.31535478 -0.31299485\n",
      " -0.29312409 -0.28239498 -0.26171729 -0.2579886  -0.24963941 -0.24924234\n",
      " -0.24000817 -0.22142693 -0.22033326 -0.21689719 -0.20745273 -0.205571\n",
      " -0.19840618 -0.19378312 -0.19068919 -0.18966751 -0.18651453 -0.18600829\n",
      " -0.18290308 -0.17951317 -0.1792467  -0.1724018  -0.1721015  -0.16579293\n",
      " -0.16147744 -0.1609297  -0.15927468 -0.15215417 -0.14830507 -0.14782004\n",
      " -0.14777711 -0.14613805 -0.14263366 -0.13720644 -0.13470159 -0.13208012\n",
      " -0.13039465 -0.12655568 -0.12512326 -0.12451442 -0.1235205  -0.12300617\n",
      " -0.12062392 -0.1204643  -0.11949878 -0.11855239 -0.11849729 -0.11712972\n",
      " -0.11689106 -0.11650956 -0.11234157 -0.10640307 -0.10560673 -0.1031513\n",
      " -0.10256049 -0.09852923 -0.0942253  -0.09279853 -0.09248011 -0.09225282\n",
      " -0.09034548 -0.09031116 -0.09000794 -0.08959836 -0.08804875 -0.0879327\n",
      " -0.08314241 -0.08233847 -0.08204105 -0.08024364 -0.07843461 -0.07587691\n",
      " -0.07503928 -0.07314909 -0.07173502 -0.07006257 -0.06949239 -0.06661785\n",
      " -0.06028638 -0.05852957 -0.05752511 -0.05734398 -0.05474796 -0.05226343\n",
      " -0.05099521 -0.0486064  -0.04259474 -0.04074033 -0.04053656 -0.0358325\n",
      " -0.03303392 -0.0320845  -0.02547608 -0.02238805 -0.01755638 -0.01571475\n",
      " -0.01516433 -0.01382268 -0.01333356 -0.01301345 -0.01114495 -0.01092525\n",
      " -0.00512638 -0.00356166 -0.0009442   0.          0.00154756  0.00252571\n",
      "  0.00313896  0.00598897  0.00634121  0.00658087  0.00899038  0.00971919\n",
      "  0.01235026  0.01436325  0.01993341  0.02025001  0.02104336  0.02202859\n",
      "  0.02472183  0.02637739  0.02649364  0.02893153  0.03293278  0.03730318\n",
      "  0.0387005   0.04059821  0.04494368  0.04507932  0.05133244  0.0624732\n",
      "  0.06619944  0.06766793  0.06860699  0.07105778  0.07257159  0.08098039\n",
      "  0.08536498  0.09375121  0.09483283  0.09558973  0.09687521  0.09860863\n",
      "  0.10874757  0.10926564  0.11155525  0.11655252  0.11999324  0.12623064\n",
      "  0.13100962  0.13239407  0.14487897  0.15239202  0.15446778  0.16831441\n",
      "  0.17280448  0.18103233  0.18195085  0.18335409  0.20819704  0.22722794\n",
      "  0.23800922  0.24089527  0.242206    0.25433884  0.30525401  0.3218051\n",
      "  0.32907571  0.34040428  0.36388551  0.41681441  0.44275823  0.49485297\n",
      "  0.54039095]\n",
      "[-3.84086601e-01 -3.58073785e-01 -3.39105902e-01 -3.29342852e-01\n",
      " -2.92308640e-01 -2.65493633e-01 -2.18180799e-01 -2.14697223e-01\n",
      " -2.09658447e-01 -2.01116373e-01 -1.97986011e-01 -1.97691755e-01\n",
      " -1.95378931e-01 -1.86271333e-01 -1.84490837e-01 -1.80938245e-01\n",
      " -1.79901183e-01 -1.79586407e-01 -1.75294187e-01 -1.71682008e-01\n",
      " -1.68478646e-01 -1.63028866e-01 -1.62737529e-01 -1.61268650e-01\n",
      " -1.59853997e-01 -1.58893601e-01 -1.57880894e-01 -1.54656457e-01\n",
      " -1.54417025e-01 -1.53443869e-01 -1.52695476e-01 -1.49738847e-01\n",
      " -1.49032309e-01 -1.46248910e-01 -1.45070043e-01 -1.43083167e-01\n",
      " -1.42728038e-01 -1.41936551e-01 -1.39327337e-01 -1.39053068e-01\n",
      " -1.38116434e-01 -1.38067119e-01 -1.37482099e-01 -1.34571376e-01\n",
      " -1.34104264e-01 -1.33612255e-01 -1.32643837e-01 -1.32297484e-01\n",
      " -1.28544002e-01 -1.27321714e-01 -1.21209504e-01 -1.18182741e-01\n",
      " -1.16651178e-01 -1.15489435e-01 -1.13360337e-01 -1.12175584e-01\n",
      " -1.08615063e-01 -1.08603649e-01 -1.08396269e-01 -1.05565430e-01\n",
      " -1.05540455e-01 -1.04590276e-01 -1.00677985e-01 -1.00452799e-01\n",
      " -9.96140763e-02 -9.87432797e-02 -9.60269398e-02 -9.41759075e-02\n",
      " -9.36741638e-02 -9.36693363e-02 -9.22796972e-02 -8.84373863e-02\n",
      " -8.16729036e-02 -7.70137593e-02 -7.60931231e-02 -7.25225968e-02\n",
      " -7.09841192e-02 -7.08028550e-02 -7.06117046e-02 -6.80981983e-02\n",
      " -6.76965891e-02 -6.74377909e-02 -6.28909220e-02 -6.00639135e-02\n",
      " -5.88449881e-02 -5.74303297e-02 -5.44736662e-02 -5.39878268e-02\n",
      " -5.27059462e-02 -4.81755182e-02 -4.76692953e-02 -4.72815812e-02\n",
      " -4.55952094e-02 -4.45021701e-02 -4.33140468e-02 -4.23604168e-02\n",
      " -4.15816762e-02 -4.13214554e-02 -3.94244655e-02 -3.70974050e-02\n",
      " -3.67809042e-02 -3.35576005e-02 -3.26816287e-02 -3.05128919e-02\n",
      " -2.87716435e-02 -2.37394007e-02 -2.33540823e-02 -2.31419137e-02\n",
      " -2.01849969e-02 -1.99359916e-02 -1.93672599e-02 -1.66682880e-02\n",
      " -1.53378787e-02 -1.36384049e-02 -1.30193846e-02 -1.26305957e-02\n",
      " -1.11995831e-02 -1.06113861e-02 -7.46354961e-03 -7.13735295e-03\n",
      " -6.04832947e-03 -4.64401407e-03 -1.52862488e-03 -1.30584008e-03\n",
      " -7.25052438e-04  0.00000000e+00  1.77881380e-05  1.30275454e-03\n",
      "  4.15947917e-03  5.05608114e-03  6.18733741e-03  8.69503224e-03\n",
      "  8.72325343e-03  9.85055037e-03  1.29618820e-02  1.58921846e-02\n",
      "  1.60217281e-02  1.85915652e-02  1.87282623e-02  2.37546509e-02\n",
      "  2.48281607e-02  2.53840795e-02  2.73791805e-02  2.88349460e-02\n",
      "  3.00381799e-02  3.13825838e-02  3.44201675e-02  3.65788097e-02\n",
      "  4.12902355e-02  4.24255984e-02  4.46517986e-02  4.86572504e-02\n",
      "  4.96038244e-02  5.52110729e-02  5.62090180e-02  6.29809763e-02\n",
      "  7.13145445e-02  7.19307925e-02  7.28165721e-02  8.50919933e-02\n",
      "  8.55364385e-02  9.41112903e-02  1.06107077e-01  1.10838380e-01\n",
      "  1.45277042e-01  1.49164737e-01  1.63787780e-01  1.69948533e-01\n",
      "  1.80243332e-01  2.29191487e-01  2.67136542e-01  3.04970864e-01\n",
      "  3.06973438e-01  3.07030299e-01  3.20667905e-01  3.26683232e-01\n",
      "  3.51019992e-01  3.94657259e-01  5.22011098e-01  5.44672978e-01\n",
      "  5.82587975e-01]\n",
      "[-0.4085807  -0.35305706 -0.34218079 -0.3331316  -0.31535478 -0.31299485\n",
      " -0.29312409 -0.28239498 -0.26171729 -0.2579886  -0.24963941 -0.24924234\n",
      " -0.24000817 -0.22142693 -0.22033326 -0.21689719 -0.20745273 -0.205571\n",
      " -0.19840618 -0.19378312 -0.19068919 -0.18966751 -0.18651453 -0.18600829\n",
      " -0.18290308 -0.17951317 -0.1792467  -0.1724018  -0.1721015  -0.16579293\n",
      " -0.16147744 -0.1609297  -0.15927468 -0.15215417 -0.14830507 -0.14782004\n",
      " -0.14777711 -0.14613805 -0.14263366 -0.13720644 -0.13470159 -0.13208012\n",
      " -0.13039465 -0.12655568 -0.12512326 -0.12451442 -0.1235205  -0.12300617\n",
      " -0.12062392 -0.1204643  -0.11949878 -0.11855239 -0.11849729 -0.11712972\n",
      " -0.11689106 -0.11650956 -0.11234157 -0.10640307 -0.10560673 -0.1031513\n",
      " -0.10256049 -0.09852923 -0.0942253  -0.09279853 -0.09248011 -0.09225282\n",
      " -0.09034548 -0.09031116 -0.09000794 -0.08959836 -0.08804875 -0.0879327\n",
      " -0.08314241 -0.08233847 -0.08204105 -0.08024364 -0.07843461 -0.07587691\n",
      " -0.07503928 -0.07314909 -0.07173502 -0.07006257 -0.06949239 -0.06661785\n",
      " -0.06028638 -0.05852957 -0.05752511 -0.05734398 -0.05474796 -0.05226343\n",
      " -0.05099521 -0.0486064  -0.04259474 -0.04074033 -0.04053656 -0.0358325\n",
      " -0.03303392 -0.0320845  -0.02547608 -0.02238805 -0.01755638 -0.01571475\n",
      " -0.01516433 -0.01382268 -0.01333356 -0.01301345 -0.01114495 -0.01092525\n",
      " -0.00512638 -0.00356166 -0.0009442   0.          0.00154756  0.00252571\n",
      "  0.00313896  0.00598897  0.00634121  0.00658087  0.00899038  0.00971919\n",
      "  0.01235026  0.01436325  0.01993341  0.02025001  0.02104336  0.02202859\n",
      "  0.02472183  0.02637739  0.02649364  0.02893153  0.03293278  0.03730318\n",
      "  0.0387005   0.04059821  0.04494368  0.04507932  0.05133244  0.0624732\n",
      "  0.06619944  0.06766793  0.06860699  0.07105778  0.07257159  0.08098039\n",
      "  0.08536498  0.09375121  0.09483283  0.09558973  0.09687521  0.09860863\n",
      "  0.10874757  0.10926564  0.11155525  0.11655252  0.11999324  0.12623064\n",
      "  0.13100962  0.13239407  0.14487897  0.15239202  0.15446778  0.16831441\n",
      "  0.17280448  0.18103233  0.18195085  0.18335409  0.20819704  0.22722794\n",
      "  0.23800922  0.24089527  0.242206    0.25433884  0.30525401  0.3218051\n",
      "  0.32907571  0.34040428  0.36388551  0.41681441  0.44275823  0.49485297\n",
      "  0.54039095]\n"
     ]
    }
   ],
   "source": [
    "scores_mean = np.append(scores_mean, np.zeros(20))\n",
    "plot_32k_results(hcp.unparcellate(scores_mean, hcp.mmp),'r2',100610,'as_scores','HCP_MMP_ridgecv_r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

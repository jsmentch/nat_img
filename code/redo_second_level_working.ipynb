{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332d56f4-00c9-476b-aba1-88f59f97a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda-latest/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import hrf_tools\n",
    "import hcp_utils as hcp\n",
    "from analysis import plot_results\n",
    "from os import walk\n",
    "from nilearn.image import load_img\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_1samp, norm\n",
    "from scipy.stats import zscore\n",
    "# subject_flist = list(walk(clean_path))[0][2:][0]\n",
    "#all_=[]\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.glm.contrasts import compute_contrast\n",
    "from nilearn.glm.first_level import run_glm\n",
    "from nilearn.image import threshold_img\n",
    "from nilearn.glm import fdr_threshold\n",
    "from scipy.stats.mstats import zscore\n",
    "from nilearn.glm.second_level import SecondLevelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f6b110-99af-4035-ad08-75401362de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm import first_level as level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0391175e-ca26-466c-a549-7f349dcc58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_second_nilearn(feature):\n",
    "    second_level_in = np.load(f'../outputs/glm/budapest/{feature}/{feature}_second_level_input.npy')\n",
    "\n",
    "    mat = np.ones(25).reshape(25,1)\n",
    "\n",
    "    labels, estimates = level1.run_glm(second_level_in, mat, noise_model='ols')\n",
    "\n",
    "    contrast = compute_contrast(labels=labels,regression_result=estimates,con_val=[1],contrast_type='t')\n",
    "\n",
    "    p_val=contrast.p_value()\n",
    "    t=contrast.stat()\n",
    "    z=contrast.z_score()\n",
    "\n",
    "    signed_neg_log_pval = np.sign(t) * -np.log10(np.minimum(1, p_val * p_val.shape[0]))\n",
    "\n",
    "    plot_results(signed_neg_log_pval,1,'raw','32k',vertex_info,'all',feature,'budapest','fwer_nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e57593-3be3-4763-b20c-2e757453b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_second(feature):\n",
    "    second_level_in = np.load(f'../outputs/glm/budapest/{feature}/{feature}_second_level_input.npy')\n",
    "    t, pval = ttest_1samp(np.nan_to_num(second_level_in), 0)\n",
    "    z_val = norm.isf(pval)\n",
    "    thresh=fdr_threshold(z_val,.05)\n",
    "\n",
    "    signed_neg_log_pval = np.sign(t) * -np.log10(np.minimum(1, pval * pval.shape[0]))\n",
    "\n",
    "    plot_results(signed_neg_log_pval,1,'raw','32k',vertex_info,'all',feature,'budapest','fwer')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5724b03-5774-4beb-b167-16e2d0028b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/vast/gablab/jsmentch/projects/nat_img/code/nilearn_plotting_custom.py:184: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n",
      "  axes = Axes3D(figure, rect=[0, 0, 1, 1],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04411913 0.0465046  0.04751332 ...        nan        nan        nan]\n",
      "[0.0909995  0.09377474 0.09534767 ...        nan        nan        nan]\n",
      "[0.04411913 0.0465046  0.04751332 ...        nan        nan        nan]\n",
      "[0.0909995  0.09377474 0.09534767 ...        nan        nan        nan]\n"
     ]
    }
   ],
   "source": [
    "run_second('speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a95bf250-0888-43c3-8348-5b0ae2a3a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.50006773 0.5000709  ...        nan        nan        nan]\n",
      "[0.5        0.50002006 0.5000218  ...        nan        nan        nan]\n",
      "[0.5        0.50006773 0.5000709  ...        nan        nan        nan]\n",
      "[0.5        0.50002006 0.5000218  ...        nan        nan        nan]\n",
      "[0.5        0.50002325 0.50002783 ...        nan        nan        nan]\n",
      "[0.5        0.50000585 0.50007376 ...        nan        nan        nan]\n",
      "[0.5        0.50002325 0.50002783 ...        nan        nan        nan]\n",
      "[0.5        0.50000585 0.50007376 ...        nan        nan        nan]\n",
      "[0.5        0.50001206 0.50006524 ...        nan        nan        nan]\n",
      "[0.5        0.50002402 0.50004759 ...        nan        nan        nan]\n",
      "[0.5        0.50001206 0.50006524 ...        nan        nan        nan]\n",
      "[0.5        0.50002402 0.50004759 ...        nan        nan        nan]\n",
      "[0.5        0.50000641 0.50000708 ...        nan        nan        nan]\n",
      "[0.5        0.50004866 0.5000527  ...        nan        nan        nan]\n",
      "[0.5        0.50000641 0.50000708 ...        nan        nan        nan]\n",
      "[0.5        0.50004866 0.5000527  ...        nan        nan        nan]\n",
      "[0.5        0.50004144 0.50005274 ...        nan        nan        nan]\n",
      "[0.5        0.50006884 0.50009026 ...        nan        nan        nan]\n",
      "[0.5        0.50004144 0.50005274 ...        nan        nan        nan]\n",
      "[0.5        0.50006884 0.50009026 ...        nan        nan        nan]\n",
      "[0.5        0.50004389 0.5000516  ...        nan        nan        nan]\n",
      "[0.5        0.50001841 0.50010791 ...        nan        nan        nan]\n",
      "[0.5        0.50004389 0.5000516  ...        nan        nan        nan]\n",
      "[0.5        0.50001841 0.50010791 ...        nan        nan        nan]\n"
     ]
    }
   ],
   "source": [
    "features = ['any_faces','speech','as_music_nospeech','as_music','any_faces_vs_home','any_faces_nospeech']\n",
    "for feature in features:\n",
    "    #run_second(feature)\n",
    "    run_second_nilearn(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7867349-1aad-4340-b924-40765d05ccd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_feature(feature):\n",
    "    speech_list = []\n",
    "    for r in np.arange(1,6):\n",
    "    #     flow = np.load(f'../sourcedata/data/budapest/features/budapest{r}_optic_flow_10hz.npy')\n",
    "    #     flow_list.append(flow)\n",
    "        data = pd.read_csv(f'../sourcedata/data/budapest/features/budapest{r}_{feature}.tsv', sep='\\t')\n",
    "        #print(data)\n",
    "        speech = np.array(data['value'])\n",
    "        speech = np.expand_dims(speech, axis=1).astype(float)\n",
    "        speech=hrf_tools.apply_optimal_hrf_10hz(speech,10)\n",
    "        speech = hrf_tools.resample_1hz(speech)\n",
    "        speech_list.append(speech)\n",
    "    return(speech_list)\n",
    "\n",
    "# clean_path = '/om/scratch/Mon/jsmentch/cleaned/smoothed/'\n",
    "## get list of subjects\n",
    "feature='speech'\n",
    "\n",
    "clean_path = '/om/scratch/Mon/jsmentch/cleaned/smoothed/'\n",
    "subject_flist = list(walk(clean_path))[0][2:][0]\n",
    "sub_list=[]\n",
    "for s in subject_flist:\n",
    "    sub = s[4:13]\n",
    "    sub_list.append(sub)\n",
    "sub_list=list(set(sub_list))\n",
    "speech_list = load_feature(feature)\n",
    "\n",
    "# second_level_input=[]\n",
    "# stat_maps=[]\n",
    "\n",
    "for i,sub in enumerate(sub_list):\n",
    "    subject = sub[0:13]\n",
    "    #if not os.path.isfile(f'../outputs/figures/budapest/part{run}_GLM_{sub}_optic_flow_z.png'):    \n",
    "    print(f'working on sub {sub}, {i+1} / {len(sub_list)+1}')\n",
    "    subject=sub\n",
    "    dataset='budapest'\n",
    "    title=f'allruns_GLM'\n",
    "    for r in np.arange(5):\n",
    "        Xi = speech_list[r]\n",
    "        img = load_img(f'{clean_path}sub-{sub}_run{r+1}_clean_smooth_space-fsLR_den-91k_bold.dtseries.nii')\n",
    "        Yi = img.get_fdata()\n",
    "        #X = np.hstack((X,X_n))\n",
    "        if r == 0:\n",
    "            Y=np.copy(Yi[10:-10,:])\n",
    "            X=np.copy(Xi)\n",
    "        else:\n",
    "            Yi=Yi[10:-10,:]\n",
    "            Xi=Xi[:-20,:]\n",
    "            X=np.vstack((X,Xi))\n",
    "            Y=np.vstack((Y,Yi))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    n_scans = Y.shape[0]\n",
    "    frame_times= np.arange(n_scans)\n",
    "\n",
    "\n",
    "    design_matrix = make_first_level_design_matrix(frame_times, None,\n",
    "                              add_regs=X, hrf_model=None, drift_model=None)\n",
    "\n",
    "#     from nilearn.plotting import plot_design_matrix\n",
    "#     plot_design_matrix(design_matrix)\n",
    "\n",
    "    labels,results = run_glm(Y,design_matrix.values)\n",
    "\n",
    "#     contrast = compute_contrast(labels=labels, \\\n",
    "#                                 regression_result=results, \\\n",
    "#                                 con_val=np.array([1,0]).T, \\\n",
    "#                                 contrast_type='t')\n",
    "    \n",
    "    \n",
    "    contrast = compute_contrast(labels=labels, \\\n",
    "                                regression_result=results, \\\n",
    "                                con_val=np.array([1,0]).T, \\\n",
    "                                contrast_type='t')\n",
    "    \n",
    "#    vertex_info = hcp.get_HCP_vertex_info(img)\n",
    "#     plot_results(contrast.z_score(),'z','32k',vertex_info,subject,feature,dataset,title)\n",
    "#     second_level_input.append(contrast.effect_size())\n",
    "#     stat_maps.append(contrast.stat())\n",
    "#     savepath = f'../outputs/glm/budapest/speech/'\n",
    "#     os.makedirs(savepath, exist_ok=True)\n",
    "#     np.save(f'{savepath}{sub}_stat',contrast.stat())\n",
    "#     contrast.stat()\n",
    "#     more_smooth_anat_img.to_filename('more_smooth_anat_img.nii.gz')\n",
    "    plot_results(contrast.z_score(),'z','32k',vertex_info,subject,feature,dataset,'partALL_GLM_effect_z')\n",
    "    gc.collect()\n",
    "# second_level_input = np.array(second_level_input)\n",
    "# savepath = f'../outputs/glm/budapest/{feature}/'\n",
    "# os.makedirs(savepath, exist_ok=True)\n",
    "# np.save(f'{savepath}{feature}_second_level_input',second_level_input)\n",
    "# np.save(f'{savepath}{feature}_stat_maps',stat_maps)\n",
    "\n",
    "# t, pval = ttest_1samp(np.nan_to_num(second_level_input), 0)\n",
    "# # z_val = norm.isf(pval)\n",
    "# z_val = zscore(t,nan_policy='omit')\n",
    "# vertex_info = hcp.get_HCP_vertex_info(img)\n",
    "\n",
    "    \n",
    "\n",
    "#np.save(f'{savepath}{feature}_zstat',z_val)\n",
    "plot_results(z_val,'z','32k',vertex_info,\"all\",feature,dataset,'partALL_GLM_effect_z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceda273c-0367-40e0-af51-7e7d2c7cc3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = load_img(f'/om/scratch/Thu/jsmentch/budapest_smoothed/cleaned/sub-sid000050_task-movie_run-3_space-fsLR_den-91k_bold_deconfound.dtseries.nii')\n",
    "vertex_info = hcp.get_HCP_vertex_info(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b390b2-a55a-4777-ac4f-ad4509427465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from nilearn_plotting_custom import plot_surf\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import npp\n",
    "import hcp_utils as hcp\n",
    "from hcp_tools import load_flatmaps_59k\n",
    "from hcp_tools import load_meshes\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sns.set(\"paper\", \"white\")\n",
    "#%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "def plot_results(scores,thresh,score_type,data_type,vertex_info,subject,feature,dataset,title):\n",
    "    '''Inputs:\n",
    "        scores = data to plot\n",
    "        threshold= thrshold level\n",
    "        score_type = r2, r, p, z, d, raw\n",
    "        data_type = 32k (3T) or 59k (7T)\n",
    "        vertex info = None or the vertex info if it is 59k data beacuse hcp_utils doesnt by default\n",
    "        subject = eg 100610 subject id for file naming\n",
    "        feature = eg as_scores plotted feature for file naming\n",
    "        dataset = eg merlin or HCP_7T which dataset?\n",
    "        title = \n",
    "    '''\n",
    "    scratch_dir = '../tmp'\n",
    "#     scratch_dir = '/scratch/scratch/Fri/jsmentch/tmp'\n",
    "#     if not os.path.exists(scratch_dir):\n",
    "#         os.mkdir(scratch_dir)\n",
    "    if score_type == 'r2':\n",
    "        v=[0,0.5]\n",
    "        threshold=None\n",
    "        symmetric_cmap=False\n",
    "        cmap='inferno'\n",
    "    if score_type == 'r':\n",
    "        v=[0,1]\n",
    "        threshold=None\n",
    "        symmetric_cmap=False\n",
    "        cmap='inferno'\n",
    "    if score_type == 'p':\n",
    "        v=[0,0.05]\n",
    "        symmetric_cmap=False\n",
    "        cmap='inferno'\n",
    "    if score_type == 'z':\n",
    "        v=[-15,15]\n",
    "        threshold=3.35\n",
    "        symmetric_cmap=True\n",
    "        cmap='cold_hot'\n",
    "    if score_type == 'd':\n",
    "        v=[0,10]\n",
    "        threshold=3\n",
    "        symmetric_cmap=True\n",
    "        cmap='inferno'\n",
    "    if score_type == 'raw':\n",
    "        v=[-20,20]\n",
    "        threshold=None\n",
    "        symmetric_cmap=True\n",
    "        cmap='cold_hot'\n",
    "    threshold=thresh\n",
    "    save_dir=f'../outputs/figures/{dataset}/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)    \n",
    "    if data_type == '59k':\n",
    "        flatmeshes=load_flatmaps_59k() #load flatmaps\n",
    "        surf_path_msm = '../sourcedata/data/human-connectome-project-openaccess/HCP1200/100610/T1w/fsaverage_LR59k/100610.L.inflated_1.6mm_MSMAll.59k_fs_LR.surf.gii'\n",
    "        mesh59k_msm = load_meshes(example_filename=surf_path_msm) #load other meshes\n",
    "        # get data from results in plotting format\n",
    "        score_cortex_dataL = hcp.left_cortex_data(scores, fill=0, vertex_info=vertex_info)\n",
    "        score_cortex_dataR = hcp.right_cortex_data(scores, fill=0, vertex_info=vertex_info)\n",
    "        # sulcal depth paths\n",
    "        sulc_left = '../sourcedata/data/human-connectome-project-openaccess/HCP1200/100610/MNINonLinear/fsaverage_LR59k/100610.L.sulc.59k_fs_LR.shape.gii'\n",
    "        sulc_right = '../sourcedata/data/human-connectome-project-openaccess/HCP1200/100610/MNINonLinear/fsaverage_LR59k/100610.R.sulc.59k_fs_LR.shape.gii'\n",
    "        # params for view to plot\n",
    "        params = [('flat_L',score_cortex_dataL,flatmeshes.flat_left,sulc_left,'left'),\\\n",
    "         ('flat_R',score_cortex_dataR,flatmeshes.flat_right,sulc_right,'right'),\\\n",
    "         ('vinf_L',score_cortex_dataL,mesh59k_msm.very_inflated_left,sulc_left,'left'),\\\n",
    "         ('vinf_R',score_cortex_dataR,mesh59k_msm.very_inflated_right,sulc_right,'right'),\\\n",
    "        ]\n",
    "    elif data_type == '32k':\n",
    "        score_cortex_dataL = hcp.left_cortex_data(scores, fill=0)\n",
    "        score_cortex_dataR = hcp.right_cortex_data(scores, fill=0)\n",
    "    #     # sulcal depth paths\n",
    "    #     sulc_left = '../sourcedata/data/human-connectome-project-openaccess/HCP1200/100610/MNINonLinear/fsaverage_LR59k/100610.L.sulc.59k_fs_LR.shape.gii'\n",
    "    #     sulc_right = '../sourcedata/data/human-connectome-project-openaccess/HCP1200/100610/MNINonLinear/fsaverage_LR59k/100610.R.sulc.59k_fs_LR.shape.gii'\n",
    "    #     # params for view to plot\n",
    "        params = [('flat_L',score_cortex_dataL,hcp.mesh.flat_left,hcp.mesh.sulc_left,'left'),\\\n",
    "         ('flat_R',score_cortex_dataR,hcp.mesh.flat_right,hcp.mesh.sulc_right,'right'),\\\n",
    "         ('vinf_L',score_cortex_dataL,hcp.mesh.very_inflated_left,hcp.mesh.sulc_left,'left'),\\\n",
    "         ('vinf_R',score_cortex_dataR,hcp.mesh.very_inflated_right,hcp.mesh.sulc_right,'right'),\\\n",
    "        ]\n",
    "    # plot each hemi and mesh, save to outputs dir\n",
    "    for name, data, mesh, sulc, hemi in params:\n",
    "        #figure, axes = plt.subplots(subplot_kw=dict(projection=\"3d\"), figsize=(6,4))\n",
    "        plot_surf(mesh,\\\n",
    "                data, \\\n",
    "                  cmap=cmap,symmetric_cmap=symmetric_cmap, avg_method='median',#figure=fig,\\\n",
    "                bg_map=sulc, colorbar=True, vmin=v[0], vmax=v[1], threshold=threshold, hemi=hemi, \\\n",
    "#                data_alpha=np.where(data>0,1,0),\\\n",
    "                data_alpha=np.ones(data.shape),\\\n",
    "                data_remove=np.zeros(data.shape),output_file=f'{scratch_dir}/{name}.png')\n",
    "#combine saved maps into one with PIL\n",
    "#     if notebook==True:\n",
    "    area = (75, 140, 635, 560) #area to crop from each image\n",
    "#     else:\n",
    "#         area = (105, 190, 880, 780)\n",
    "        \n",
    "    img = Image.open(f'{scratch_dir}/flat_L.png',mode='r')\n",
    "    img = img.resize((770,720))\n",
    "    cropped = img.crop(area)\n",
    "    fL=cropped.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    w,h = img.size\n",
    "    c_area = (690, 0, w-10, h) # area of colorbar to crop\n",
    "    cbar = img.crop(c_area)\n",
    "\n",
    "    img = Image.open(f'{scratch_dir}/flat_R.png',mode='r')\n",
    "    img = img.resize((770,720))\n",
    "    fR = img.crop(area)\n",
    "\n",
    "    img = Image.open(f'{scratch_dir}/vinf_L.png',mode='r')\n",
    "    img = img.resize((770,720))\n",
    "    iL = img.crop(area)\n",
    "    #iL=cropped.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    img = Image.open(f'{scratch_dir}/vinf_R.png',mode='r')\n",
    "    img = img.resize((770,720))\n",
    "    iR = img.crop(area)\n",
    "\n",
    "    w,h=iR.size\n",
    "\n",
    "    new_im = Image.new('RGB', ( (w*2)+70 , h*2) ,(255, 255, 255, 1))\n",
    "    new_im.paste(fL,(0,h))\n",
    "    new_im.paste(fR,(w,h))\n",
    "    new_im.paste(iL,(0,0))\n",
    "    new_im.paste(iR,(w,0))\n",
    "    new_im.paste(cbar,(w*2,int(round(h/4))))\n",
    "\n",
    "    w,h=new_im.size\n",
    "\n",
    "    draw = ImageDraw.Draw(new_im)\n",
    "    draw.text((0,0),f\"{title}_{subject}_{feature}_{score_type}\",(0,0,0))\n",
    "\n",
    "    new_im.save(f'{save_dir}{title}_{subject}_{feature}_{score_type}.png')\n",
    "#     os.remove(f'{scratch_dir}/flat_L.png')\n",
    "#     os.remove(f'{scratch_dir}/flat_R.png')\n",
    "#     os.remove(f'{scratch_dir}/vinf_L.png')\n",
    "#     os.remove(f'{scratch_dir}/vinf_R.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b3328-46a4-4a7a-af06-c55220cef90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
